
Learning Compact Representations for Efficient Reinforcement Learning in Simulated Environments

This project investigates whether reinforcement learning (RL) agents can learn faster and more reliably when trained on compact latent representations rather than raw pixel observations.
We explore how representation learningâ€”specifically using Convolutional Autoencodersâ€”can improve sample efficiency, stability, and generalization in RL tasks.
The work is implemented in MiniGrid, a lightweight and interpretable RL environment.

Team Members: Meriem Jelassi Â· Supriya Malla Â· Sydney Nzunguli

â¸»

1. ğŸ” Motivation

Deep RL has achieved strong results on visual tasks, but raw observations are often high-dimensional and redundant.
This leads to:
	â€¢	slow and unstable training
	â€¢	poor sample efficiency
	â€¢	limited generalization

Learning compact latent representations offers a potential solution.
Autoencoders or contrastive models can extract meaningful, low-dimensional features that capture the essential structure of an environment while removing irrelevant details.

This project explores whether using such latent embeddings can help RL agents train more efficiently and achieve more robust behavior.

â¸»

2. ğŸ¯ Problem Definition

Research Question:
Can an RL agent learn faster and generalize better when using compact latent representations instead of raw visual inputs?

Raw pixel frames often include unnecessary details.
Latent spaces, on the other hand:
	â€¢	compress important features
	â€¢	remove noise
	â€¢	represent the underlying structure of the scene

Our goal is to evaluate whether RL performance improves when using these compact representations.

â¸»

3. ğŸ§  Methodology

A. Representation Learning Stage
	1.	Collect state images from MiniGrid environments
	2.	Train a Convolutional Autoencoder to compress frames into low-dimensional vectors
	3.	Assess the quality of the latent space using:
	â€¢	t-SNE / UMAP visualizations
	â€¢	linear probing

B. Reinforcement Learning Stage
	1.	Freeze (or optionally fine-tune) the pretrained encoder
	2.	Use the latent vector output as input to a DQN agent
	3.	Train a baseline DQN directly on raw pixels for comparison
	4.	Analyze:
	â€¢	sample efficiency
	â€¢	stability across training
	â€¢	final reward performance

C. Analysis
	â€¢	Compare learning curves between the latent-based agent and the baseline
	â€¢	Visualize the structure of learned latent spaces
	â€¢	Interpret whether representation learning contributes to more stable policies

â¸»

4. ğŸ“ˆ Expected Results

Agents trained on compact latent representations are expected to:
	â€¢	converge faster
	â€¢	achieve higher rewards with fewer interactions
	â€¢	learn more stable and transferable policies
	â€¢	exhibit structured latent spaces with semantically meaningful features
(e.g., agent position, object layout, navigational cues)

These findings would show how representation learning can improve the efficiency and robustness of RL systems.

â¸»

5. ğŸ› ï¸ Keywords

Convolutional Neural Networks Â· Autoencoder Â· Representation Learning Â· Reinforcement Learning Â· Contrastive Learning Â· Regularization Â· Generalization

â¸»

6. ğŸ“‚ Repository Structure

.
â”œâ”€â”€ autoencoder/          # Model definition and training scripts
â”œâ”€â”€ rl_agent/             # DQN agent trained on latent vectors
â”œâ”€â”€ experiments/          # Evaluation, visualizations, tests
â”œâ”€â”€ utils/                # Environment wrappers and helpers
â””â”€â”€ README.md


â¸»

7. ğŸš€ How to Run

Train the Autoencoder

python autoencoder/train_autoencoder.py

Train the DQN Agent

python rl_agent/train_rl.py

The RL script loads the pretrained encoder and trains the agent on latent states.

â¸»

8. ğŸ‘¤ About the Project

This work was part of a research-oriented learning project focused on improving efficiency in reinforcement learning. Our aim was to combine ideas from deep representation learning and RL to study how learned latent spaces can support more efficient adaptive decision-making.
